{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Student Information**\n",
    "Name:  蔡宗達\n",
    "\n",
    "Student ID: 111020001\n",
    "\n",
    "GitHub ID: da111020001-lab\n",
    "\n",
    "Kaggle name: daOoOab\n",
    "\n",
    "Kaggle private scoreboard snapshot: \n",
    "\n",
    "![pic_ranking.png](./pics/pic_ranking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Instructions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab we have divided the assignments into **three phases/parts**. The `first two phases` refer to the `exercises inside the Master notebooks` of the [DM2025-Lab2-Exercise Repo](https://github.com/difersalest/DM2025-Lab2-Exercise.git). The `third phase` refers to an `internal Kaggle competition` that we are gonna run among all the Data Mining students. Together they add up to `100 points` of your grade. There are also some `bonus points` to be gained if you complete `extra exercises` in the lab **(bonus 15 pts)** and in the `Kaggle Competition report` **(bonus 5 pts)**.\n",
    "\n",
    "**Environment recommendations to solve lab 2:**\n",
    "- **Phase 1 exercises:** Need GPU for training the models explained in that part, if you don't have a GPU in your laptop it is recommended to run in Colab or Kaggle for a faster experience, although with CPU they can still be solved but with a slower execution.\n",
    "- **Phase 2 exercises:** We use Gemini's API so everything can be run with only CPU without a problem.\n",
    "- **Phase 3 exercises:** For the competition you will probably need GPU to train your models, so it is recommended to use Colab or Kaggle if you don't have a laptop with a dedicated GPU.\n",
    "- **Optional Ollama Notebook (not graded):** You need GPU, at least 4GB of VRAM with 16 GB of RAM to run the local open-source LLM models. \n",
    "\n",
    "## **Phase 1 (30 pts):**\n",
    "\n",
    "1. __Main Exercises (25 pts):__ Do the **take home exercises** from Sections: `1. Data Preparation` to `9. High-dimension Visualization: t-SNE and UMAP`, in the [DM2025-Lab2-Master-Phase_1 Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_1.ipynb). Total: `8 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 3th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "## **Phase 2 (30 pts):**\n",
    "\n",
    "1. **Main Exercises (25 pts):** Do the remaining **take home exercises** from Section: `2. Large Language Models (LLMs)` in the [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb). Total: `5 exercises required from sections 2.1, 2.2, 2.4 and 2.6`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "3. **`Bonus (15 pts):`** Complete the bonus exercises in the [DM2025-Lab2-Master-Phase_2_Bonus Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Bonus.ipynb) and [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb) `where 2 exercises are counted as bonus from sections 2.3 and 2.5 in the main notebook`. Total: `7 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "## **Phase 3 (40 pts):**\n",
    "\n",
    "1. **Kaggle Competition Participation (30 pts):** Participate in the in-class **Kaggle Competition** regarding Emotion Recognition on Twitter by clicking in this link: **[Data Mining Class Kaggle Competition](https://www.kaggle.com/t/3a2df4c6d6b4417e8bf718ed648d7554)**. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20 pts of the 30 pts in this competition participation part.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**. Make sure to take a screenshot of your position at the end of the competition and store it as `pic_ranking.png` under the `pics` folder of this repository and rerun the cell **Student Information**.\n",
    "\n",
    "2. **Competition Report (10 pts)** A report section to be filled in inside this notebook in Markdown Format, we already provided you with the template below. You need to describe your work developing the model for the competition. The report should include a section describing briefly the following elements: \n",
    "* Your preprocessing steps.\n",
    "* The feature engineering steps.\n",
    "* Explanation of your model.\n",
    "\n",
    "* **`Bonus (5 pts):`**\n",
    "    * You will have to describe more detail in the previous steps.\n",
    "    * Mention different things you tried.\n",
    "    * Mention insights you gained. \n",
    "\n",
    "[Markdown Guide - Basic Syntax](https://www.markdownguide.org/basic-syntax/)\n",
    "\n",
    "**`Things to note for Phase 3:`**\n",
    "\n",
    "* **The code used for the competition should be in this Jupyter Notebook File** `DM2025-Lab2-Homework.ipynb`.\n",
    "\n",
    "* **Push the code used for the competition to your repository**.\n",
    "\n",
    "* **The code should have a clear separation for the same sections of the report, preprocessing, feature engineering and model explanation. Briefly comment your code for easier understanding, we provide a template at the end of this notebook.**\n",
    "\n",
    "* Showing the kaggle screenshot of the ranking plus the code in this notebook will ensure the validity of your participation and the report to obtain the corresponding points.\n",
    "\n",
    "After the competition ends you will have two days more to submit the `DM2025-Lab2-Homework.ipynb` with your report in markdown format and your code. Do everything **`BEFORE the deadline (Nov. 26th, 11:59 pm, Wednesday) to obtain 100% of the available points.`**\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding NTU Cool assignment.\n",
    "\n",
    "## **Deadlines:**\n",
    "\n",
    "![lab2_deadlines](./pics/lab2_deadlines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next you will find the template report with some simple markdown syntax explanations, use it to structure your content.\n",
    "\n",
    "You can delete the syntax suggestions after you use them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Project Report**\n",
    "\n",
    "## 1. Model Development (10 pts Required)\n",
    "\n",
    "### 1.1 Preprocessing Steps\n",
    "I performed a series of preprocessing steps to clean the noisy Twitter data while preserving sentiment-bearing structures:\n",
    "* **Lowercasing**: Converted all text to lowercase to maintain consistency across the dataset.\n",
    "* **Noise Removal**: Used Regular Expressions (RegEx) to remove non-textual elements: URLs, user mentions (`@user`), hashtags (`#`), punctuation, and numbers.\n",
    "* **Stemming**: Applied `PorterStemmer` to reduce words to their root form (e.g., \"running\" $\\rightarrow$ \"run\"), reducing the vocabulary size.\n",
    "* **Smart Stopword Removal**:\n",
    "    * Instead of removing all standard English stopwords, I **customized the list** to **retain negative words** (e.g., *\"not\", \"no\", \"never\", \"can't\"*).\n",
    "    * **Reasoning**: Removing these words often flips the sentiment (e.g., \"not happy\" becomes \"happy\"), so keeping them is crucial for accuracy.\n",
    "\n",
    "### 1.2 Feature Engineering Steps\n",
    "I utilized a **Feature Union** strategy to capture different aspects of the text simultaneously:\n",
    "\n",
    "* **View 1: Word-Level TF-IDF**\n",
    "    * **Configuration**: `ngram_range=(1, 2)`, `max_features=20000`.\n",
    "    * **Purpose**: Captures semantic meaning through words and common bigrams (phrases).\n",
    "* **View 2: Char-Level TF-IDF**\n",
    "    * **Configuration**: `ngram_range=(3, 5)`, `max_features=10000`, `analyzer='char_wb'`.\n",
    "    * **Purpose**: Captures morphological patterns and informal writing styles common in tweets (e.g., elongated words like \"sooooo\" or \"happppy\"). The `char_wb` analyzer ensures that character n-grams respect word boundaries.\n",
    "\n",
    "### 1.3 Explanation of Your Model\n",
    "I implemented an **Ensemble Model** using a **Voting Classifier** to combine the strengths of two different algorithms:\n",
    "\n",
    "1. **Model 1: Logistic Regression** (`class_weight='balanced'`)\n",
    "    * A robust linear model that performs well on high-dimensional text data. The balanced weight helps handle the uneven distribution of emotions.\n",
    "2. **Model 2: Multinomial Naive Bayes** (`alpha=0.1`)\n",
    "    * A probabilistic model that is particularly effective for text classification tasks based on word counts.\n",
    "\n",
    "* **Ensemble Strategy: Soft Voting**\n",
    "    * I used `voting='soft'` with weights `[1.5, 1]` favoring Logistic Regression. This averages the predicted probabilities of both models, resulting in a more stable and generalized prediction than using a single model alone.\n",
    "* **Pipeline**: The entire process (Feature Union $\\rightarrow$ Voting Classifier) is wrapped in a `sklearn.pipeline.Pipeline` to prevent data leakage during cross-validation.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Bonus Section (5 pts Optional)\n",
    "\n",
    "### 2.1 Mention Different Things You Tried\n",
    "During the development process, I experimented with several approaches before finalizing the model:\n",
    "* **Baseline Model**: I started with a simple Logistic Regression using only unigrams. It provided a decent baseline but failed to capture complex sentence structures.\n",
    "* **Single Model vs. Ensemble**: I tried using a standalone **LinearSVC**. While it had high accuracy, the **Voting Classifier (LR + NB)** provided better probability estimates and generalization on the validation set.\n",
    "* **Stopword Variations**: I initially removed all stopwords. However, after noticing the model struggled with negated sentences (e.g., \"I am not sad\"), I modified the preprocessing to strictly keep negation words.\n",
    "\n",
    "### 2.2 Mention Insights You Gained\n",
    "* **The Power of Negation**: The most significant insight was that standard NLP preprocessing can hurt sentiment analysis if not done carefully. Retaining words like \"not\" and \"no\" is essential for correct classification.\n",
    "* **Character N-grams for Noisy Text**: Traditional word-based models often treat \"happy\" and \"happppy\" as completely different unknown words. By adding **Character N-grams (3-5 grams)**, the model successfully learned that \"ppp\" or repeated characters often intensify the emotion, allowing it to handle the informal nature of Twitter data effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**`From here on starts the code section for the competition.`**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Competition Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在進行深度解析與組裝...\n",
      "JSON 解析完成！成功轉換 64171 筆貼文\n",
      "偵測到 Train/Test 標記欄位為: 'split'，將其統一命名為 'identification'\n",
      "正在合併資料...\n",
      "------------------------------\n",
      "資料組裝成功！\n",
      "訓練集: 47890 筆\n",
      "測試集: 16281 筆\n",
      "訓練集欄位: ['tweet_id', 'identification', 'content', 'sentiment']\n",
      "\n",
      "資料預覽:\n",
      "   tweet_id identification                                            content  \\\n",
      "0  0x35663e          train  I bet there is an army of married couples who ...   \n",
      "1  0xc78afe          train                         This could only end badly.   \n",
      "2  0x90089c          train  My sister squeezed a lime in her milk when she...   \n",
      "\n",
      "  sentiment  \n",
      "0       joy  \n",
      "1      fear  \n",
      "2       joy  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 設定檔案路徑\n",
    "path_json = 'dm-lab-2-private-competition/final_posts.json'\n",
    "path_id = 'dm-lab-2-private-competition/data_identification.csv'\n",
    "path_emotion = 'dm-lab-2-private-competition/emotion.csv'\n",
    "\n",
    "print(\"正在解析與組裝...\")\n",
    "\n",
    "# 1. 讀取 JSON 並手動挖掘資料\n",
    "with open(path_json, 'r', encoding='utf-8') as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "parsed_data = []\n",
    "for entry in raw_data:\n",
    "    try:\n",
    "        post_info = entry['root']['_source']['post']\n",
    "        row = {'tweet_id': post_info['post_id'], 'content': post_info['text']}\n",
    "        parsed_data.append(row)\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "df_posts = pd.DataFrame(parsed_data)\n",
    "print(f\"JSON 解析完成！成功轉換 {len(df_posts)} 筆貼文\")\n",
    "\n",
    "# 2. 讀取 CSV\n",
    "df_id = pd.read_csv(path_id, encoding='utf-8-sig')\n",
    "df_emotion = pd.read_csv(path_emotion, encoding='utf-8-sig')\n",
    "\n",
    "# --- 強力欄位清洗函式 ---\n",
    "def clean_and_unify_columns(df, name):\n",
    "    # 1. 去除空白\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    \n",
    "    # 2. 處理 ID 欄位 (通常是第一欄)\n",
    "    if 'tweet_id' not in df.columns:\n",
    "        first_col = df.columns[0]\n",
    "        df.rename(columns={first_col: 'tweet_id'}, inplace=True)\n",
    "        \n",
    "    return df\n",
    "\n",
    "df_id = clean_and_unify_columns(df_id, \"ID表\")\n",
    "df_emotion = clean_and_unify_columns(df_emotion, \"情緒表\")\n",
    "\n",
    "# 偵測 identification 欄位 \n",
    "possible_id_cols = ['identification', 'dataset', 'split', 'type']\n",
    "target_col = None\n",
    "\n",
    "# 找identification\n",
    "for col in df_id.columns:\n",
    "    if col in possible_id_cols:\n",
    "        target_col = col\n",
    "        break\n",
    "\n",
    "# 找不到，檢查哪一欄的內容包含 'train' 或 'test'\n",
    "if target_col is None:\n",
    "    for col in df_id.columns:\n",
    "        if df_id[col].astype(str).str.contains('train|test', case=False).any():\n",
    "            target_col = col\n",
    "            break\n",
    "\n",
    "if target_col:\n",
    "    print(f\"偵測到 Train/Test 標記欄位為: '{target_col}'，將其統一命名為 'identification'\")\n",
    "    df_id.rename(columns={target_col: 'identification'}, inplace=True)\n",
    "else:\n",
    "    # 找不到，印出所有欄位\n",
    "    print(f\"錯誤：在 ID 表中找不到區分 train/test 的欄位！目前的欄位: {df_id.columns.tolist()}\")\n",
    "    # 假設第二欄是 identification (通常第一欄是 ID)\n",
    "    if len(df_id.columns) > 1:\n",
    "        print(\"嘗試將第二欄強制設為 identification\")\n",
    "        df_id.rename(columns={df_id.columns[1]: 'identification'}, inplace=True)\n",
    "\n",
    "# 3. 合併資料\n",
    "print(\"正在合併資料...\")\n",
    "df_full = df_id.merge(df_posts, on='tweet_id', how='left')\n",
    "\n",
    "# 4. 拆分 Train 和 Test\n",
    "df_train = df_full[df_full['identification'] == 'train'].copy()\n",
    "df_test = df_full[df_full['identification'] == 'test'].copy()\n",
    "\n",
    "# 5. 合併情緒標籤\n",
    "df_train = df_train.merge(df_emotion, on='tweet_id', how='left')\n",
    "\n",
    "# 整理\n",
    "if 'emotion' in df_train.columns:\n",
    "    df_train.rename(columns={'emotion': 'sentiment'}, inplace=True)\n",
    "\n",
    "df_test.rename(columns={'tweet_id': 'id'}, inplace=True)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\"資料組裝成功！\")\n",
    "print(f\"訓練集: {len(df_train)} 筆\")\n",
    "print(f\"測試集: {len(df_test)} 筆\")\n",
    "print(f\"訓練集欄位: {df_train.columns.tolist()}\")\n",
    "\n",
    "# 最後檢查一下內容\n",
    "print(\"\\n資料預覽:\")\n",
    "print(df_train.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\da314\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\da314\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preprocessing...\n",
      "Preprocessing done! Cleaned content created.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# 若已下載過可註解掉\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text) # 去網址\n",
    "    text = re.sub(r'\\@\\w+|\\#', '', text)                 # 去 @user, #hashtag\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) # 去標點\n",
    "    text = re.sub(r'\\d+', '', text)                      # 去數字\n",
    "    \n",
    "    tokens = text.split()\n",
    "    tokens = [stemmer.stem(w) for w in tokens if w not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "print(\"Start preprocessing...\")\n",
    "# 你的資料欄位叫 'content'，這裡對應修正\n",
    "df_train['clean_content'] = df_train['content'].apply(preprocess_text)\n",
    "df_test['clean_content'] = df_test['content'].apply(preprocess_text)\n",
    "print(\"Preprocessing done! Cleaned content created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\da314\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\da314\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start smarter preprocessing (keeping negatives)...\n",
      "Preprocessing done!\n"
     ]
    }
   ],
   "source": [
    "# --- 修正版 Preprocessing (保留否定詞) ---\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# 1. 取得標準停用詞表\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# 2. 把否定詞從停用詞表中移除\n",
    "negative_words = {'not', 'no', 'never', 'nor', 'neither', 'none', 'cannot', \"n't\", \"don't\", \"won't\"}\n",
    "# 從 set 中扣除這些字\n",
    "stop_words = stop_words - negative_words\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'\\@\\w+|\\#', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    tokens = text.split()\n",
    "    # 這裡的 stop_words 已經不包含 'not' 了，所以否定詞會被保留\n",
    "    tokens = [stemmer.stem(w) for w in tokens if w not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "print(\"Start smarter preprocessing (keeping negatives)...\")\n",
    "df_train['clean_content'] = df_train['content'].apply(preprocess_text)\n",
    "df_test['clean_content'] = df_test['content'].apply(preprocess_text)\n",
    "print(\"Preprocessing done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Engineering (Word + Char TF-IDF)。\n",
      "資料型態: <class 'sklearn.pipeline.FeatureUnion'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Feature Union\n",
    "combined_features = FeatureUnion([\n",
    "    # A: Word 抓關鍵意涵\n",
    "    ('word_tfidf', TfidfVectorizer(\n",
    "        ngram_range=(1, 2),      # 看單字和雙字詞\n",
    "        max_features=20000,      # 保留更多特徵\n",
    "        min_df=2,\n",
    "        analyzer='word',\n",
    "        stop_words=None          # 前面 Step 1 已經處理過停用詞\n",
    "    )),\n",
    "    \n",
    "    # B: Char 抓拼寫習慣與語氣\n",
    "    ('char_tfidf', TfidfVectorizer(\n",
    "        ngram_range=(3, 5),      # 抓 3 到 5 個字母的片段\n",
    "        max_features=10000,\n",
    "        analyzer='char_wb',      # char_wb 會考慮單字邊界\n",
    "        min_df=2\n",
    "    )),\n",
    "])\n",
    "\n",
    "# 配合 Pipeline 防止洩漏，只準備 Raw Text，不做 fit_transform\n",
    "X_all_content = df_train['clean_content'].astype(str)\n",
    "y_all_labels = df_train['sentiment']\n",
    "X_test_content = df_test['clean_content'].astype(str)\n",
    "\n",
    "print(\"Feature Engineering (Word + Char TF-IDF)。\")\n",
    "print(f\"資料型態: {type(combined_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Implementation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練 Word + Char 模型...\n",
      "Validation Macro F1 Score: 0.4454\n",
      "\n",
      "分類報告:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.53      0.53      0.53      1077\n",
      "     disgust       0.18      0.20      0.19       113\n",
      "        fear       0.35      0.48      0.40       189\n",
      "         joy       0.77      0.72      0.75      2397\n",
      "     sadness       0.40      0.39      0.40       386\n",
      "    surprise       0.39      0.43      0.41       627\n",
      "\n",
      "    accuracy                           0.59      4789\n",
      "   macro avg       0.44      0.46      0.45      4789\n",
      "weighted avg       0.61      0.59      0.60      4789\n",
      "\n",
      "全資料重新訓練...\n",
      "submission_ensemble_char.csv done!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# Ensemble\n",
    "# 結合 Logistic Regression 和 MultinomialNB\n",
    "clf1 = LogisticRegression(max_iter=2000, class_weight='balanced', solver='lbfgs', C=1.0)\n",
    "clf2 = MultinomialNB(alpha=0.1)\n",
    "\n",
    "ensemble_clf = VotingClassifier(\n",
    "    estimators=[('lr', clf1), ('nb', clf2)],\n",
    "    voting='soft',       # 使用機率加權投票\n",
    "    weights=[1.5, 1]     # 稍微加重 LR 的權重\n",
    ")\n",
    "\n",
    "# Pipeline\n",
    "# 串接 Section 2 定義的 combined_features 和這裡定義的模型\n",
    "pipeline = Pipeline([\n",
    "    ('features', combined_features), \n",
    "    ('classifier', ensemble_clf)\n",
    "])\n",
    "\n",
    "# 切分驗證集\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_all_content, y_all_labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# 訓練與驗證\n",
    "print(\"訓練 Word + Char 模型...\")\n",
    "pipeline.fit(X_tr, y_tr)\n",
    "\n",
    "val_preds = pipeline.predict(X_val)\n",
    "val_f1 = f1_score(y_val, val_preds, average='macro')\n",
    "print(f\"Validation Macro F1 Score: {val_f1:.4f}\")\n",
    "print(\"\\n分類報告:\")\n",
    "print(classification_report(y_val, val_preds))\n",
    "\n",
    "# 全資料重新訓練\n",
    "print(\"全資料重新訓練...\")\n",
    "pipeline.fit(X_all_content, y_all_labels)\n",
    "final_preds = pipeline.predict(X_test_content)\n",
    "\n",
    "# Submission\n",
    "submission = pd.DataFrame({\n",
    "    'id': df_test['id'],\n",
    "    'emotion': final_preds\n",
    "})\n",
    "submission.to_csv('submission_ensemble_char.csv', index=False)\n",
    "print(\"submission_ensemble_char.csv done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM2025-Lab2-Exercise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
